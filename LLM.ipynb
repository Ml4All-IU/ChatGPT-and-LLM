{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xongeNOGlUiM",
        "outputId": "60e8a9be-330e-478f-a722-b6ad04edd20a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.1)\n",
            "Requirement already satisfied: spaCy in /usr/local/lib/python3.10/dist-packages (3.6.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spaCy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spaCy) (1.0.4)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spaCy) (1.0.9)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spaCy) (2.0.7)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spaCy) (3.0.8)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spaCy) (8.1.12)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spaCy) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spaCy) (2.4.7)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spaCy) (2.0.9)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spaCy) (0.9.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spaCy) (0.10.2)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spaCy) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spaCy) (4.66.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spaCy) (1.23.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spaCy) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spaCy) (1.10.12)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spaCy) (3.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spaCy) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spaCy) (23.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spaCy) (3.3.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spaCy) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spaCy) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spaCy) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spaCy) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spaCy) (2023.7.22)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spaCy) (0.7.10)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spaCy) (0.1.2)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spaCy) (8.1.7)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spaCy) (2.1.3)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.23.5)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.11.2)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.2.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.7.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.1.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.42.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.5)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (23.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
            "Requirement already satisfied: wordcloud in /usr/local/lib/python3.10/dist-packages (1.9.2)\n",
            "Requirement already satisfied: numpy>=1.6.1 in /usr/local/lib/python3.10/dist-packages (from wordcloud) (1.23.5)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from wordcloud) (9.4.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from wordcloud) (3.7.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->wordcloud) (1.1.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->wordcloud) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->wordcloud) (4.42.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->wordcloud) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->wordcloud) (23.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->wordcloud) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->wordcloud) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->wordcloud) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install nltk\n",
        "!pip install spaCy\n",
        "!pip install scikit-learn\n",
        "!pip install matplotlib\n",
        "!pip install wordcloud\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize"
      ],
      "metadata": {
        "id": "mKlj5bf_lcIt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "text = \"Natural Language Processing is a subfield of artificial intelligence.\"\n",
        "text_new = \"Hello my name is Tri\"\n",
        "\n",
        "# Tokenize by words\n",
        "words = word_tokenize(text)\n",
        "words_2 = word_tokenize(text_new)\n",
        "# Tokenize by sentences\n",
        "sentences = sent_tokenize(text)\n",
        "sentences_2 = word_tokenize(text_new)\n",
        "print(\"Words:\", words)\n",
        "print(\"Sentences:\", sentences)\n",
        "\n",
        "print(\"Words:\", words_2)\n",
        "print(\"Sentences:\", sentences_2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hPSdFq9GlqAI",
        "outputId": "f4ee84e6-59f9-47f1-ae77-eb7976f8541e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Words: ['Natural', 'Language', 'Processing', 'is', 'a', 'subfield', 'of', 'artificial', 'intelligence', '.']\n",
            "Sentences: ['Natural Language Processing is a subfield of artificial intelligence.']\n",
            "Words: ['Hello', 'my', 'name', 'is', 'Tri']\n",
            "Sentences: ['Hello', 'my', 'name', 'is', 'Tri']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "\n",
        "nltk.download('stopwords')\n",
        "\n",
        "\n",
        "text = \"This is an example sentence with some stop words.\"\n",
        "\n",
        "# Remove stop words\n",
        "stop_words = set(stopwords.words('english'))\n",
        "print(\"stopwords: \", stop_words)\n",
        "filtered_words = [word for word in word_tokenize(text) if word.lower() not in stop_words]\n",
        "\n",
        "print(\"Filtered Words:\", filtered_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FPQooXBOrub-",
        "outputId": "93498118-d95f-4bad-800c-f35e082e073f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "stopwords:  {'whom', \"she's\", 'the', 'if', 'itself', 'this', \"it's\", 're', 'are', 'did', 'not', 'again', 'such', \"hadn't\", \"that'll\", 'then', 'we', 'down', 've', 'themselves', 'where', 'is', 'wouldn', 'aren', 'through', 'your', \"mustn't\", 'yours', 'when', 'these', 'by', \"wasn't\", 'him', 'should', 'that', 'into', 'after', 'a', 'too', \"you'll\", \"you'd\", \"shouldn't\", 'was', 'what', 'hasn', 'any', 'yourselves', 'do', 'on', 'd', 'haven', 'our', 'them', 'each', 'during', 'more', 'been', 'having', 'under', 'she', 'it', 'against', 'those', 'while', 's', 'don', 'only', 't', 'doing', 'from', 'hadn', 'needn', 'above', 'in', 'didn', 'being', 'all', \"shan't\", 'theirs', 'just', \"aren't\", 'you', 'why', 'doesn', 'but', 'be', 'ain', 'wasn', 'an', \"you're\", 'same', 'up', 'my', 'both', 'will', 'and', 'about', 'shan', 'yourself', 'hers', \"haven't\", 'i', 'were', \"don't\", 'for', 'to', 'have', 'because', 'me', 'himself', 'very', 'with', 'ma', 'off', \"didn't\", 'does', 'they', 'until', 'or', 'm', 'other', 'couldn', \"isn't\", 'here', 'his', 'nor', 'own', 'over', \"wouldn't\", 'won', 'out', \"needn't\", 'their', \"doesn't\", 'which', 'once', \"you've\", 'her', 'before', \"mightn't\", \"won't\", 'who', 'few', 'further', 'weren', 'mightn', 'so', 'than', 'y', \"couldn't\", 'some', 'its', 'ours', 'can', 'll', 'myself', 'at', 'below', 'of', 'herself', 'has', 'am', \"hasn't\", 'ourselves', 'had', 'between', 'most', 'mustn', 'how', 'isn', 'shouldn', 'no', \"should've\", 'now', 'o', 'there', 'as', \"weren't\", 'he'}\n",
            "Filtered Words: ['example', 'sentence', 'stop', 'words', '.']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "stemmer = PorterStemmer()\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "word = \"running\"\n",
        "word_work = \"working\"\n",
        "word_history = \"history\"\n",
        "word_remove = \"remove\"\n",
        "stemmed_word = stemmer.stem(word)\n",
        "lemmatized_word = lemmatizer.lemmatize(word_work, pos=\"v\")\n",
        "\n",
        "stemmed_word_1 = stemmer.stem(word_history)\n",
        "lemmatized_word_1 = lemmatizer.lemmatize(word_history, pos=\"v\")\n",
        "\n",
        "stemmed_word_2 = stemmer.stem(word_remove)\n",
        "lemmatized_word_2 = lemmatizer.lemmatize(word_remove, pos=\"v\")\n",
        "\n",
        "\n",
        "print(\"Stemmed Word:\", stemmed_word)\n",
        "print(\"Lemmatized Word:\", lemmatized_word)\n",
        "\n",
        "\n",
        "print(\"Stemmed Word:\", stemmed_word_1)\n",
        "print(\"Lemmatized Word:\", lemmatized_word_1)\n",
        "\n",
        "\n",
        "print(\"Stemmed Word:\", stemmed_word_2)\n",
        "print(\"Lemmatized Word:\", lemmatized_word_2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B2XC9pzmr8Ob",
        "outputId": "4184393f-c777-4600-96f9-dbbe8dcdf924"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stemmed Word: run\n",
            "Lemmatized Word: work\n",
            "Stemmed Word: histori\n",
            "Lemmatized Word: history\n",
            "Stemmed Word: remov\n",
            "Lemmatized Word: remove\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "# Sample text data and labels\n",
        "texts =[\"This is a positive message.\", #positve\n",
        "        \"Negative sentiment here.\", #Negative\n",
        "        \"I love this product!\",  # positive\n",
        "        \"This movie is fantastic.\", #positive\n",
        "        \"The weather today is awful.\",\n",
        "        \"I can't stand this place.\",\n",
        "        \"My day was great!\",\n",
        "        \"The service was terrible.\",\n",
        "        \"The food was amazing.\",\n",
        "        \"I had a horrible experience.\",\n",
        "        \"I'm feeling happy today.\",\n",
        "        \"The quality of this product is poor.\",\n",
        "        \"I'm excited to try this out.\",\n",
        "        \"The results were disappointing.\"]\n",
        "\n",
        "labels = [\"positive\",\n",
        "          \"negative\",\n",
        "          \"positive\",\n",
        "          \"positive\",\n",
        "          \"negative\",\n",
        "          \"negative\",\n",
        "          \"positive\",\n",
        "          \"negative\",\n",
        "          \"positive\",\n",
        "          \"negative\",\n",
        "          \"positive\",\n",
        "          \"negative\",\n",
        "          \"positive\",\n",
        "          \"negative\"]\n",
        "\n",
        "# Create a CountVectorizer\n",
        "vectorizer = CountVectorizer()\n",
        "\n",
        "# Transform the text data into a feature matrix\n",
        "X = vectorizer.fit_transform(texts)\n",
        "\n",
        "# Create a Naive Bayes classifier\n",
        "classifier = MultinomialNB()\n",
        "\n",
        "# Train the classifier\n",
        "classifier.fit(X, labels)\n",
        "\n",
        "# Predict sentiment for new text\n",
        "new_text = [\"I don't like cars\"]\n",
        "\n",
        "X_new = vectorizer.transform(new_text)\n",
        "predicted_label = classifier.predict(X_new)\n",
        "\n",
        "print(\"Predicted Label:\", predicted_label)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9FV4LHJPs-R3",
        "outputId": "c7d943b4-b192-4f47-a208-cead49b43935"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted Label: ['negative']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "essay = \"Once upon a time in a faraway land, there lived a young and curious explorer named Alice. She had a passion for adventure and a desire to uncover hidden mysteries. One day, while wandering through a dense forest, she stumbled upon an ancient map that led to a forgotten treasure. With excitement in her heart, Alice embarked on a journey filled with challenges and encounters with strange creatures. Along the way, she made loyal friends who helped her overcome obstacles. As she delved deeper into the unknown, Alice's determination and courage grew stronger. In the end, she not only found the treasure but also discovered the true value of friendship and the beauty of the world around her.\""
      ],
      "metadata": {
        "id": "0Ug8UWsL9Rmk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.probability import FreqDist\n",
        "\n",
        "freq_dist = FreqDist()\n",
        "\n"
      ],
      "metadata": {
        "id": "Q8nExSA79FYS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "essay_token = word_tokenize(essay)\n",
        "essay_token"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JEmBbaZjbTY3",
        "outputId": "e1ba5c0b-eaec-4082-dac5-f5d4e8e45a96"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Once',\n",
              " 'upon',\n",
              " 'a',\n",
              " 'time',\n",
              " 'in',\n",
              " 'a',\n",
              " 'faraway',\n",
              " 'land',\n",
              " ',',\n",
              " 'there',\n",
              " 'lived',\n",
              " 'a',\n",
              " 'young',\n",
              " 'and',\n",
              " 'curious',\n",
              " 'explorer',\n",
              " 'named',\n",
              " 'Alice',\n",
              " '.',\n",
              " 'She',\n",
              " 'had',\n",
              " 'a',\n",
              " 'passion',\n",
              " 'for',\n",
              " 'adventure',\n",
              " 'and',\n",
              " 'a',\n",
              " 'desire',\n",
              " 'to',\n",
              " 'uncover',\n",
              " 'hidden',\n",
              " 'mysteries',\n",
              " '.',\n",
              " 'One',\n",
              " 'day',\n",
              " ',',\n",
              " 'while',\n",
              " 'wandering',\n",
              " 'through',\n",
              " 'a',\n",
              " 'dense',\n",
              " 'forest',\n",
              " ',',\n",
              " 'she',\n",
              " 'stumbled',\n",
              " 'upon',\n",
              " 'an',\n",
              " 'ancient',\n",
              " 'map',\n",
              " 'that',\n",
              " 'led',\n",
              " 'to',\n",
              " 'a',\n",
              " 'forgotten',\n",
              " 'treasure',\n",
              " '.',\n",
              " 'With',\n",
              " 'excitement',\n",
              " 'in',\n",
              " 'her',\n",
              " 'heart',\n",
              " ',',\n",
              " 'Alice',\n",
              " 'embarked',\n",
              " 'on',\n",
              " 'a',\n",
              " 'journey',\n",
              " 'filled',\n",
              " 'with',\n",
              " 'challenges',\n",
              " 'and',\n",
              " 'encounters',\n",
              " 'with',\n",
              " 'strange',\n",
              " 'creatures',\n",
              " '.',\n",
              " 'Along',\n",
              " 'the',\n",
              " 'way',\n",
              " ',',\n",
              " 'she',\n",
              " 'made',\n",
              " 'loyal',\n",
              " 'friends',\n",
              " 'who',\n",
              " 'helped',\n",
              " 'her',\n",
              " 'overcome',\n",
              " 'obstacles',\n",
              " '.',\n",
              " 'As',\n",
              " 'she',\n",
              " 'delved',\n",
              " 'deeper',\n",
              " 'into',\n",
              " 'the',\n",
              " 'unknown',\n",
              " ',',\n",
              " 'Alice',\n",
              " \"'s\",\n",
              " 'determination',\n",
              " 'and',\n",
              " 'courage',\n",
              " 'grew',\n",
              " 'stronger',\n",
              " '.',\n",
              " 'In',\n",
              " 'the',\n",
              " 'end',\n",
              " ',',\n",
              " 'she',\n",
              " 'not',\n",
              " 'only',\n",
              " 'found',\n",
              " 'the',\n",
              " 'treasure',\n",
              " 'but',\n",
              " 'also',\n",
              " 'discovered',\n",
              " 'the',\n",
              " 'true',\n",
              " 'value',\n",
              " 'of',\n",
              " 'friendship',\n",
              " 'and',\n",
              " 'the',\n",
              " 'beauty',\n",
              " 'of',\n",
              " 'the',\n",
              " 'world',\n",
              " 'around',\n",
              " 'her',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for word in essay_token:\n",
        "  freq_dist[word.lower()] += 1\n",
        "freq_dist"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wA675QJ0cbi-",
        "outputId": "053428a7-113c-497c-8677-8a8dcb82aa70"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "FreqDist({'a': 8, ',': 7, '.': 7, 'the': 7, 'and': 5, 'she': 5, 'in': 3, 'alice': 3, 'with': 3, 'her': 3, ...})"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fdist_top10 = freq_dist.most_common(5)\n",
        "fdist_top10"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cR7Lo7SYb5d2",
        "outputId": "9328d199-1ab1-4061-b200-c8d01671dee2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('a', 8), (',', 7), ('.', 7), ('the', 7), ('and', 5)]"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('averaged_perceptron_tagger')\n",
        "sentence = \"I like rainbow because it is colorful\"\n",
        "sent_token = word_tokenize(sentence)\n",
        "\n",
        "for token in sent_token:\n",
        "  print(nltk.pos_tag([token]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W70DW_z2cV0s",
        "outputId": "bb392c6e-67ed-4ba4-fdc3-03f27dd2d264"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('I', 'PRP')]\n",
            "[('like', 'IN')]\n",
            "[('rainbow', 'NN')]\n",
            "[('because', 'IN')]\n",
            "[('it', 'PRP')]\n",
            "[('is', 'VBZ')]\n",
            "[('colorful', 'NN')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import ne_chunk\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')\n",
        "NE_sent = \"Google's CEO Sundar Pichai introduced Bard in 2023\"\n",
        "NE_token = word_tokenize(NE_sent)\n",
        "NE_tags = nltk.pos_tag(NE_token)\n",
        "NE_NER = ne_chunk(NE_tags)\n",
        "print(NE_NER)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "upj--bA5dhhl",
        "outputId": "37ad1359-c17e-4820-a192-1c75a8475609"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(S\n",
            "  (GPE Google/NNP)\n",
            "  's/POS\n",
            "  (ORGANIZATION CEO/NNP Sundar/NNP Pichai/NNP)\n",
            "  introduced/VBD\n",
            "  (PERSON Bard/NNP)\n",
            "  in/IN\n",
            "  2023/CD)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai"
      ],
      "metadata": {
        "id": "bImUsj3XdoGy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e16d54e7-acf7-434e-dbe4-53407dd191ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai\n",
            "  Downloading openai-0.28.1-py3-none-any.whl (76 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/77.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.0/77.0 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai) (3.8.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2023.7.22)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.3.1)\n",
            "Installing collected packages: openai\n",
            "Successfully installed openai-0.28.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "\n",
        "openai.api_key = \"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\"\n",
        "\n",
        "response = openai.Completion.create(\n",
        "    engine=\"text-davinci-003\",\n",
        "    prompt = \"Who is Ronaldo?\",\n",
        "    max_tokens = 60\n",
        ")\n",
        "print(response.choices[0].text)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kOe5jgU9Wltu",
        "outputId": "610f9f0e-a6cf-4c48-b051-630714e887e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Ronaldo is a soccer player who is considered to be one of the greatest of all time. He currently plays for Juventus and the Portuguese national team. He is a five-time Ballon d'Or winner and has won league titles in England, Spain, and Italy.\n"
          ]
        }
      ]
    }
  ]
}